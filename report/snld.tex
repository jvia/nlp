
\documentclass[12pt]{article}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{times}
\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{alltt}
\usepackage{wrapfig}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\setlength{\topmargin}{0 mm}
\setlength{\headsep}{0 mm}
\setlength{\headheight}{0 in}
\setlength{\voffset}{0 mm}
\setlength{\oddsidemargin}{0 mm}
\setlength{\evensidemargin}{0 mm}
\setlength{\hoffset}{0 mm}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{9 in}

\title{Introspection Guided Dialog-Based Task Resolution}
\author{Jeremiah Via}

\begin{document}
\maketitle
\begin{abstract}
  Situated task-based dialog poses many challenges for traditional
  natural language understanding techniques. Key among these are
  ambiguous reference resolution, incomplete utterances, task-specific
  utterances. This paper presents a probabilistic belief framework for
  dealing with these challenges. By representing beliefs about the
  world, the agent's own capabilities, and the capabilities of other
  agents in the environment, a dialog manager can choose actions
  which have the highest likelihood of increasing common ground. A
  probabilistic approach has the additional benefit of remaining
  robust to contradictions, allowing the dialog manager to simply
  update its probabilities.
\end{abstract}

\section{Introduction}
\label{sec:intro}
In situated contexts, natural language takes on a different set of
characteristics than it does in the form in which it is typically
studied---namely prose. This makes much research not fully-applicable
to situated contexts, where natural language understanding software
must deal with various types of disfluency, common-ground alignment,
and response-time requirements \cite{Scheutz2011:dialogue}. For this
particular project, the problem of ambiguous referents, ungrammatical
sentences, and incrementality were studied. A probabilistic belief
framework was designed to exploit situatedness to better handle these
challenges.

During natural dialog, ambiguous referents are almost certain to
appear. This occurs because humans make a number of hidden assumptions
when conversing. For example, person A might ask person B to hand them
``the mug'' from a table with multiple mugs, with the hidden
assumption that ``the mug'' refers to the one closest to person
B. These kind of underspecified referents need to be handled by a
natural language understanding system. Using situatedness in this case
yields a clear benefit. In the example with the mug, one could include
in their model that ambiguous referents are likely to be grounded to
the specific instance closest to the speaker or listener.

Human dialog, unlike prose, is full of ungrammatical
sentences. Examples include answers in response to questions or
exclamations. Dealing with these types of sentences requires
maintaining one's own context and the model of the other agent's
context. In other words, when an agent makes a partial statement, what
is the context in which this agent is most likely operating. 

Incrementality is a natural part of speech. Rarely is a task between
two human laid out in full detail in one statement. Typically, the
meaning of a task is negotiated between two humans, with a
back-and-forth dialog. This process ensures common ground between
the agents. To function successfully with humans, a robot must be able
to have partial task representations which are refined through
dialog until enough information is present to complete the task.

The rest of the paper is organized as follows. Section
\ref{sec:scenario} introduces the scenario used for this
project. Particular attention will be paid to the real-world aspects
of the domain that make traditional natural language
understanding. This scenario will motivate the necessity of exploiting
the embodied nature of the robot and its situatedness for improved
performance in this kind of environment. Section \ref{sec:framework}
will describe the approach used with respect to the problem domain. A
belief manager using a probabilistic belief representation is proposed
to best make use of situatedness for directing dialog and physical
action with respect to the task.  Particular difficulties of
task-based dialog will be used as examples against the idea to show
how performance improves. In section \ref{sec:analysis} an analysis of
the approach will be made.

\section{Scenario}
\label{sec:scenario}
% Describe the scenario/task that you are considering in detail
For this task, a robot is placed in an office environment with a
communication link to two human agents, known as \textit{cmdrX} and
\textit{cmdrY}. The goal of this task is for the robot to locate a
specific medical kit. No one agent has complete knowledge of the
environment or task, nor is this knowledge guaranteed to be accurate.
As a further difficulty, there are multiple medical kits in the
office, and a specific one must be determined for the task to be
completed. Key to this task are the inter-mixing of dialog and
physical action, underspecified instructions, and belief revision.

% make reference to all the sources you used and inform the reader
% about possible strategies for developing algorithms for the task
The robot has beliefs about the usual location of these kinds of
medical kits, but when reality fails to meet expectations, it must
gather information from two interlocutors participating in the
task. Neither interlocutor has full knowledge of the task and so the
robot must direct the conversation such that it can maximize its
knowledge related to the task.

There are two main approaches to handling dialog
management---knowledge-based approaches and data-driven approaches.
Knowledge-based approaches typically used a set of hard-coded rules
represented by a finite state automaton
\cite{Lee2010:dialog-management}. These are useful in
highly-structured tasks with a restricted vocabulary. While the
ability to represent a dialog in a finite state automaton is appealing
for efficiency and human-understandability, they are a suboptimal
choice for situated, natural dialog because of the inability to handle
unknown utterances or unexpected dialog flow. Data-driven approaches
utilize sets of annotated corpora to train reinforcement learning
algorithms, e.g., partially observable Markov decision processes
\cite{Young2013:dialog-pomdp}. While training and annotation are time
consuming, the algorithm is easily applied to new data sets making an
algorithm more widely applicable. General challenges with these
approaches is hand-turning them post-training to better fit the
dialog. There is also the challenge that the learned model does not
change over time. This is particularly troubling in a situated
context, where unexpected information about a task can appear at any
time, making a more flexible approach desirable. There is work on
hybridizing these approaches which has shown promise.

\section{An Introspective Belief Framework}
\label{sec:framework}
% Describe in detail your solution to the problem, possibly including
% graphs and figures to aid the reader Include detailed descriptions of
% your the algorithm and how the situated context is utilized to make
% parts of the NL chain work better (this is critical and should be the
% highlight of the paper)
To handle the challenges previously outlined, an introspective
framework utilizing probabilistic beliefs was developed. This
framework made use of ADE \cite{Kramer2007} as a cognitive
architecture sitting on top of the ROS middleware \cite{Quigley2009}.
Central to this idea was a constantly changing set of beliefs. The
goal of the agent was to minimize the entropy of its beliefs with
respect to the task. This belief structure was used to augment a
simple, rule-based dialog manager, allowing it reach conclusions
otherwise not possible. The algorithm ran in a reactive manner to new
utterances, using each utterance to update its set of beliefs and then
using this new belief set and the utterance to generate an appropriate
action. The action type was varied, ranging from basic motion
commands, to navigation commands, to dialog acts. The two key
algorithms were \texttt{update} and \texttt{act}.

\begin{algorithm}
  \DontPrintSemicolon
  \KwData{an utterance $u$}
  \KwResult{$beliefs'$}
  \Switch{$u$}{
    \Case{instruction}{
      addGoal(instruction)\;
      \If {instruction not ground} {
        add subgoals to ground instruction
      }
    }
    \Case{statement}{
      \If {clarification} {
        ground referent
      }
      \If {correction} {
        resolve contradictions
      }
      \If {agent assessment} {
        update agent confidence
      }
    }
    \Case{exclamation}{
      lower self confidence\;
      attempt to determine possible problem\;
    }
    \Case{question}{
      search belief for most likely answer\;
    }
  }
  \caption{\texttt{UPDATE}---Update the belief structure.}
  \label{algo:update}
\end{algorithm}

The \texttt{update} algorithm (algorithm \ref{algo:update}) takes
as input a new speaker utterance and updates its beliefs about the
task, its interlocutors, the world, or itself. The manner of this
update is dependent on the type of utterance. For example,
instructions are placed as goals in the belief structure, with
sub-goals being added as necessary. Sub-goals are created to ground
ambiguous referents in an utterance. Clarifying statements are used to
change belief probabilities about, e.g., to which ground referent an
ambiguous referent refers or the locations of an object in the
environment. An exclaiming utterance is a particularly challenging
type of utterance. If the robot is performing an utterance and its
interlocutor exclaims, the robot must figure out what is wrong with
respect to the environment and its actions. Doing this in a general
way would be an interesting area of further research. For this study,
the safest maneuver determined was to stop the current action and wait
further instruction. When asked a question, the robot must search its
belief structure to find the most likely answer. For example, if asked
``What are you doing?'', the robot must look at its current
action. Failing to find any reasonable answer, the robot may answer
``I don't know.''

The \texttt{act} algorithm (algorithm \ref{algo:action}) uses the
current utterance and newly updated beliefs to choose an action. This
action may be a dialog act or a physical action. Given an instruction
utterance, \texttt{act} checks to see if the generated goals are
ground. If so, it begins to execute the plan. If not, it must
construct a question which will disambiguate between multiple possible
referents. Similarly, when given a question, the belief structure is
consulted for the answer which is generated by \texttt{update}. This
answer is then transformed into a more grammatical utterance which is
then vocalized. For this study, statements had an effect on the belief
structure only and did not require any specific action, so a simple
acknowledgment to keep the interlocutor updated was sufficient.

\begin{algorithm}
  \DontPrintSemicolon
  \KwData{an utterance $u$}
  \KwResult{$action$}
    \Switch{$u$}{
    \Case{instruction}{
      \If{ground(goals)} {
        execute action\;
      } 
      \Else {
        ask clarification question to most capable agent\;
      }
    }
    \Case{statement}{
      acknowledge
    }\Case{exclamation}{
      stop(current action)\;
    }
    \Case{question}{
      answer with most likely answer from beliefs\;
    }
  }
  \caption{ \texttt{ACTION}---Choose an appropriate action.}
  \label{algo:action}
\end{algorithm}

The key point of the algorithms proposed are the use of probabilistic
beliefs. This makes it more robust to natural dialogue, especially
when composed of partial utterances, non sequiturs, or partially
understood statements. Fusing beliefs about the world with utterances
and other sensor data allows a robotic agent to better perform
task-based dialogue. With a high-level understanding of these two core
algorithms, we now give example traces through small dialog acts. The
intent is to show how these algorithms use situatedness to handle the
challenges shown previously in section \ref{sec:intro}.

\subsection{Common Ground}
For task-based dialog, it is critical that all agents involved refer
to the same objects in the environment when using a shared
label. Otherwise, confusion ensues and task completion is
jeopardized. In natural dialog, the process of creating consistent
labels for objects is one of negotiation. For a robot to take part in
this negotiation, it must be able to fuse beliefs about the world, its
perception, and utterances from its interlocutor. Doing this
probabilistically allows an agent to handle the uncertainty present in
this kind of negotiation, effectively allowing it to determine what
the interlocutors most likely mean.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
\begin{verbatim}
      cmdrX: find the medkit.
      self:  the red_medkit or
             the blu_medkit?
      cmdrX: the red_medkit.
      self:  okay.
\end{verbatim}    
    \label{fig:cg_text}
    \caption{Dialog}
  \end{subfigure}
  ~\qquad\qquad
  \begin{subfigure}{0.4\textwidth}
    \centering
\begin{verbatim}
{:referents
  {:medkit {:red_medkit 0.9 
            :blu_medkit 0.1}}}


\end{verbatim}
    \label{fig:cg_beliefs}
    \caption{Beliefs}
  \end{subfigure}
  \caption{A dialog action requiring common ground. At first the
    referent ``medkit'' is ambiguous given the medical kits of which
    it has knowledge, so the robot must take action to ground this
    referent.}
  \label{fig:cg}
\end{figure}

In figure \ref{fig:cg}, an example of this process can be seen. The
interlocutor, \textit{cmdrX}, commands the agent to find a medical
kit. Because the agent has knowledge of more than one medical kit, the
referent is ambiguous. In this case the agent simply asks to which
ground referent the interlocutor refers. This idea can be extended to
more complex cases where, e.g., the agent must examine multiple
candidate objects and create a disambiguating phrase based on the most
salient feature of each object. With this the ability, the agent could
ask ``The red medkit on the table or the large medkit down the hall?''
This sort of meaning negotiation is important to task-based dialog,
and without situatedness the negotiation process would either take a
long time or be impossible.

\subsection{Surprise}
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
cmdrX: whoa!
cmdrX: what are you doing?
self:  going to areaA to
       find the red_medkit.
cmdrX: the red_medkit is not
       at areaA.



\end{verbatim}
    \label{fig:surprise_text}
    \caption{Dialog}
  \end{subfigure}
  ~\qquad\qquad
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
{:self {:confidence 0.5}}
{:red_medkit
  {:at {:areaA 0.33
        :areaB 0.33
        :areaC 0.33}}
  {:type {:medkit 1.0}}}
{:referents
  {:medkit [:red_medkit 0.9 
            :blu_medkit 0.1]}}
\end{verbatim}
    \label{fig:surprise_beliefs}
    \caption{Beliefs}
  \end{subfigure}
  \caption{The interlocutor exclaiming causes the robot to lose
    self-confidence. After further dialog, the robot is unsure of the
    location of the medical kit.}
  \label{fig:surprise}
\end{figure}

Handling interlocutor surprise is of particular importance in an
embodied context. It is necessary for task efficiency and human and
robot safety. In our case (shown in figure \ref{fig:surprise}), simple
surprise handling was used. If a surprising statement occurred during
action execution, the robot would stop its current action and decrease
its self confidence. With less self-confidence, input from its
interlocutors would have more of an effect. For example, if the agent
had a low self-confidence value and a high confidence value for its
interlocutor, it would be more willing to accept the interlocutors
assertion of an object's location instead of its sensors. It also
could end up moving more slowly through the environment if it was
unsure of its actions. Our use of these values were more simplistic
and use simply to determine whether or not to continue action
execution, but one can see how a more general version could
effectively used situatedness to confer more natural interactability
to a robot.

\subsection{Entropy Reduction}

For task efficiency, a robot should attempt to remove as much
uncertainty from task-related knowledge as it can. In the example
shown in figure \ref{fig:er}, the robot is unsure of the location of
the red medical kit. Its first attempt to resolve this is to ask its
current interlocutor, \textit{cmdrX}, about the location. Because
\textit{cmdrX} does not know the location of the red medical kit, the
robot updates its confidence of \textit{cmdrX} and asks the new most
capable agent. In this case, that is \textit{cmdrY}. The robot chooses
this action because it believes that asking \textit{cmdrY} will give
it the most information with respect to the medical kit location. In
other instances, the robot may believe that exploration is the
efficient path to increasing its knowledge of the world. A more
generalized version of this idea would take consider when its worth to
turn on certain types of sensors, e.g., a camera to detect colored
objects being referenced in dialog. Conversely, the robot should also
take into account what it believes the other agents know, and use this
information when generating utterances. In this view, the robot has
beliefs about the other agents' own uncertainty and can direct dialog
in a direction to reduce it. One can see that entropy reduction is a
more general case of common grounding and handling surprise, a both
processes reduce uncertainty in the belief structure.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
self:  where is the red_medkit?
cmdrX: i don't know.
self:  cmdrY, where is the
       red_medkit?
cmdrY: areaB.
self:  okay.


\end{verbatim}
    \label{fig:er_text}
    \caption{Dialog}
  \end{subfigure}
  ~\qquad\qquad
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
{:self {:confidence 0.5}}
{:red_medkit
   {:at  {:areaA 0.1
          :areaB 0.8
          :areaC 0.1}
    :type :medkit}}
{:cmdrX {:confidence 0.6}}
{:cmdrY {:confidence 0.9}}
\end{verbatim}
    \label{fig:er_beliefs}
    \caption{Beliefs}
  \end{subfigure}
  \caption{In this dialog snippet, the robotic agent, upon learning
    that \textit{cmdrX} does not know where the red medical kit is
    located, updates its beliefs and proceeds to ask the agent it now
    beliefs is most likely to know.}
  \label{fig:er}
\end{figure}

\subsection{Analysis}
\label{sec:analysis}
% Discuss the advantages and possible disadvantages of your algorithm
% Suggest additional experiments and/or improvements to the solution.
As with any design choice, there are a number of trade-offs. The
initial advantage lies in the ability to make use of and have beliefs
about a given situation.  Compared to knowledge-based approaches,
there is more flexibility but with a loss of predictability. By having
a probabilistic take on the properties pattern \cite{Fowler}, the
belief structure is at any point extensible and modifiable. This is
great freedom and especially use for any agent remaining in a situated
environment over an extended duration.  There is a downside with an
endlessly extensible belief structure. Because the beliefs can change
in ways not predicted at design, optimal actions from the robot's
perspective differ from human expectation.

Data-driven approaches share the use of a probabilistic
representation, making them good candidates for embodied
contexts. They also have the advantage that they are single algorithms
whose behavior, in the case of the POMDP for example, are completely
determined by generating a policy from training data. The downside is
that this training is done offline, and modifying the policy online as
context changes is an unsolved problem. This may be a fine limitation
for certain tasks, but other tasks may require a robot to be deployed
for an extended duration. In this case, being able to incorporate new
experiences is important and this is where a probabilistic belief
framework excels.

Beyond the main challenges of the property pattern, there is the
additional burden of how to perform automatic inference across these
belief structures. As it stands now, beliefs are updated manually,
with the programmer determining which beliefs, if they exist, are most
relevant to updating another belief. This is both tedious and error
prone, so finding a way to automate it would greatly enhance the
abilities of the agent and free the programmer from exhaustive
specification.

\section{Conclusion}
\label{sec:conclusion}
% Summarize what your solution achieved, and how your algorithm could
% be extended or used in other tasks Briefly discuss directions for
% future work.

Using situated contexts and probabilistic frameworks for managing
dialogue and action is an area with further room for growth.
Utilizing more types of probabilistic distributions would allow for
more robust inference processes. In this study, only the Bernoulii
distribution and the more general Categorical distribution were
used. This is quite limiting when reasoning about objects in space,
e.g., the location of a given medical kit. A better approach would be
to use continuous distributions, perhaps backed by a particle filter.
Another direction would be to direct dialog to increase common ground
between all agents in the environment. For example, during the dialog,
after the robotic agent first received the location of the red medical
kit from cmdrX, it could have tried to create common ground with
cmdrY, creating an immediate contradiction between the beliefs of
cmdrX and cmdrY. Dialog could then pursue a resolution to this
dialog. A final improvement could be performed during the grounding of
ambiguous referents. In this example, the robot simply chooses a
feature with separate values to attempt to disambiguate the potential
referents. A more natural approach would be to choose the most salient
features of the objects for comparison. This would require having
beliefs about objects and their salient features, but it is a natural
fit for this kind of belief framework.

This paper gave an overview on how the usage of situatedness and
probabilistic belief representations can enhance task
performance. Probabilistic representations are a flexible approach to
fuse information from multiple sources into a coherent set of
beliefs. By mixing sensor information with statements from an
interlocutor and beliefs about itself and its interlocutors, a robot
can choose actions which are most likely to accomplish its goals. By
more quickly resolving ambiguity or focusing on sources most likely to
contain needed knowledge, a robot can act more naturally with its
interlocutors as it attempts to complete its goals.

\bibliographystyle{apalike}{\small \bibliography{references}}
\end{document}
% LocalWords:  disfluencies disfluency incrementality  cmdrY
% LocalWords:  cmdrX situatedness underspecified probabilistically
%  LocalWords:  disambiguating interactability
