
\documentclass[12pt]{article}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{times}
\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{alltt}
\usepackage{wrapfig}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\setlength{\topmargin}{0 mm}
\setlength{\headsep}{0 mm}
\setlength{\headheight}{0 in}
\setlength{\voffset}{0 mm}
\setlength{\oddsidemargin}{0 mm}
\setlength{\evensidemargin}{0 mm}
\setlength{\hoffset}{0 mm}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{9 in}

\title{Introspection Guided Dialog-Based Task Resolution}
\author{Jeremiah Via}

\begin{document}
\maketitle
\begin{abstract}
  Situated task-based dialog poses many challenges for traditional
  natural language understanding techniques. Key among these are
  ambiguous reference resolution, incomplete utterances, task-specific
  utterances. This paper presents a probabilistic belief framework for
  dealing with these challenges. By representing beliefs about the
  world, the agent's own capabilities, and the capabilities of other
  agents in the environment, a dialog manager can choose actions
  which have the highest likelihood of increasing common ground. A
  probabilistic approach has the additional benefit of remaining
  robust to contradictions, allowing the dialog manager to simply
  update its probabilities.
\end{abstract}

\section{Introduction}
\label{sec:intro}
In situated contexts, natural language takes on a different set of
characteristics than it does in the form in which it is typically
studied---namely prose. This makes much research not fully-applicable
to situated contexts, where natural language understanding software
must deal with various types of disfluency, common-ground alignment,
and response-time requirements \cite{Scheutz2011:dialogue}. For this
particular project, the problem of ambiguous referents, ungrammatical
sentences, and incrementality were studied. A probabilistic belief
framework was designed to exploit situatedness to better handle these
challenges.

During natural dialog, ambiguous referents are almost certain to
appear. This occurs because humans make a number of hidden assumptions
when conversing. For example, person A might ask person B to hand them
``the mug'' from a table with multiple mugs, with the hidden
assumption that ``the mug'' refers to the one closest to person
B. These kind of underspecified referents need to be handled by a
natural language understanding system. Using situatedness in this case
yields a clear benefit. In the example with the mug, one could include
in their model that ambiguous referents are likely to be grounded to
the specific instance closest to the speaker or listener.

Human dialog, unlike prose, is full of ungrammatical
sentences. Examples include answers in response to questions or
exclamations. Dealing with these types of sentences requires
maintaining one's own context and the model of the other agent's
context. In other words, when an agent makes a partial statement, what
is the context in which this agent is most likely operating. 

Incrementality is a natural part of speech. Rarely is a task between
two human laid out in full detail in one statement. Typically, the
meaning of a task is negotiated between two humans, with a
back-and-forth dialog. This process ensures common ground between
the agents. To function successfully with humans, a robot must be able
to have partial task representations which are refined through
dialog until enough information is present to complete the task.

The rest of the paper is organized as follows. Section
\ref{sec:scenario} introduces the scenario used for this
project. Particular attention will be paid to the real-world aspects
of the domain that make traditional natural language
understanding. This scenario will motivate the necessity of exploiting
the embodied nature of the robot and its situatedness for improved
performance in this kind of environment. Section \ref{sec:framework}
will describe the approach used with respect to the problem domain. A
belief manager using a probabilistic belief representation is proposed
to best make use of situatedness for directing dialog and physical
action with respect to the task.  Particular difficulties of
task-based dialog will be used as examples against the idea to show
how performance improves. In section \ref{sec:analysis} an analysis of
the approach will be made.

\section{Scenario}
\label{sec:scenario}
% Describe the scenario/task that you are considering in detail
For this task, a robot is placed in an office environment with a
communication link to two human agents, known as \textit{cmdrX} and
\textit{cmdrY}. The goal of this task is for the robot to locate a
specific medical kit. No one agent has complete knowledge of the
environment or task, nor is this knowledge guaranteed to be accurate.
As a further difficulty, there are multiple medical kits in the
office, and a specific one must be determined for the task to be
completed. Key to this task are the inter-mixing of dialog and
physical action, underspecified instructions, and belief revision.

% make reference to all the sources you used and inform the reader
% about possible strategies for developing algorithms for the task
The robot has beliefs about the usual location of these kinds of
medical kits, but when reality fails to meet expectations, it must
gather information from two interlocutors participating in the
task. Neither interlocutor has full knowledge of the task and so the
robot must direct the conversation such that it can maximize its
knowledge related to the task.

There are two main approaches to handling dialog
management---knowledge-based approaches and data-driven approaches.
Knowledge-based approaches typically used a set of hard-coded rules
represented by a finite state automaton
\cite{Lee2010:dialog-management}. These are useful in
highly-structured tasks with a restricted vocabulary. While the
ability to represent a dialog in a finite state automaton is appealing
for efficiency and human-understandability, they are a suboptimal
choice for situated, natural dialog because of the inability to handle
unknown utterances or unexpected dialog flow. Data-driven approaches
utilize sets of annotated corpora to train reinforcement learning
algorithms, e.g., partially observable Markov decision processes
\cite{Young2013:dialog-pomdp}. While training and annotation are time
consuming, the algorithm is easily applied to new data sets making an
algorithm more widely applicable. General challenges with these
approaches is hand-turning them post-training to better fit the
dialog. There is also the challenge that the learned model does not
change over time. This is particularly troubling in a situated
context, where unexpected information about a task can appear at any
time, making a more flexible approach desirable. There is work on
hybridizing these approaches which has shown promise.

\section{An Introspective Belief Framework}
\label{sec:framework}
% Describe in detail your solution to the problem, possibly including
% graphs and figures to aid the reader Include detailed descriptions of
% your the algorithm and how the situated context is utilized to make
% parts of the NL chain work better (this is critical and should be the
% highlight of the paper)
To handle the challenges previously outlined, an introspective
framework utilizing probabilistic beliefs was developed. Central to
this idea was a constantly changing set of beliefs. The goal of the
agent was to minimize the entropy of its beliefs with respect to the
task. This belief structure was used to augment a simple, rule-based
dialog manager, allowing it reach conclusions otherwise not
possible. The algorithm ran in a reactive manner to new utterances,
using each utterance to update its set of beliefs and then using this
new belief set and the utterance to generate an appropriate
action. The action type was varied, ranging from basic motion
commands, to navigation commands, to dialog acts. The two key
algorithms were \texttt{update} and \texttt{act}.

The \texttt{update} algorithm (algorithm \pageref{algo:update}) takes
as input a new speaker utterance and updates its beliefs about the
task, its interlocutors, the world, or itself. The manner of this
update is dependent on the type of utterance. For example,
instructions are placed as goals in the belief structure, with
sub-goals being added as necessary. Sub-goals are created to ground
ambiguous referents in an utterance. Clarifying statements are used to
change belief probabilities about, e.g., to which ground referent an
ambiguous referent refers or the locations of an object in the
environment. An exclaiming utterance is a particularly challenging
type of utterance. If the robot is performing an utterance and its
interlocutor exclaims, the robot must figure out what is wrong with
respect to the environment and its actions. Doing this in a general
way would be an interesting area of further research. For this study,
the safest maneuver determined was to stop the current action and wait
further instruction.

The \texttt{act} algorithm (algorithm \ref{algo:action})

% Key points: algorithm uses probabilistic belief, making it more robust
% to the problems posed by situatedness. depending on its confidence
% about beliefs, it asks questions it believes will yield the most
% information to the interlocutor it believes can yield the most
% information.

\begin{algorithm}{h}
  \DontPrintSemicolon
  \KwData{an utterance $u$}
  \KwResult{$beliefs'$}
  \Switch{$u$}{
    \Case{instruction}{
      addGoal(instruction)\;
      \If {instruction not ground} {
        add subgoals to ground instruction
      }
    }
    \Case{statement}{
      \If {clarification} {
        ground referent
      }
      \If {correction} {
        resolve contradictions
      }
      \If {agent assessment} {
        update agent confidence
      }
    }
    \Case{exclamation}{
      lower self confidence\;
      attempt to determine possible problem\;
    }
  }
  \caption{Update Beliefs}
  \label{algo:update}
\end{algorithm}


\begin{algorithm}
  \DontPrintSemicolon
  \KwData{$utterance, beliefs$}
  \KwResult{$action$}
  awesome stuff here
  \caption{Action}
  \label{algo:action}
\end{algorithm}

With high-level understanding of these two core algorithms, we now
give example traces through small dialog acts. The intent is to show
how these algorithms use situatedness to handle the challenges shown
previously in section \ref{sec:intro}. 

\subsection{Common Ground}
For task-based dialog, it is critical that all agents involved refer
to the same objects in the environment when using a share
label. Otherwise, confusion ensues and task completion jeopardized. In
natural dialog, the process of creating consistent labels for objects
is one of negotiation. 

\begin{figure}
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
\begin{verbatim}
      cmdrX: find the medkit.
      self:  the red_medkit or
             the blu_medkit?
      cmdrX: the red_medkit.
      self:  okay.
\end{verbatim}    
    \label{fig:cg_text}
    \caption{Dialog}
  \end{subfigure}
  ~\quad
  \begin{subfigure}{0.4\textwidth}
    \centering
\begin{verbatim}
      {:referents
        {:medkit [:red_medkit 0.9 
                  :blu_medkit 0.1]}}
\end{verbatim}
    \label{fig:cg_beliefs}
    \caption{Beliefs}
  \end{subfigure}
  \caption{A dialog action requiring common ground. At first the
    referent ``medkit'' is ambiguous given the medical kits of which
    it has knowledge, so the robot must take action to ground this
    referent.}
  \label{fig:cg}
\end{figure}

\subsection{Surprise}
\begin{figure}
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
cmdrX: whoa!
cmdrX: what are you doing?
self:  going to areaA to
       find the red_medkit.
cmdrX: the red_medkit is not
       at areaA.
\end{verbatim}
    \label{fig:surprise_text}
    \caption{Dialog}
  \end{subfigure}
  ~\quad
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
{:self {:confidence 0.5
        :location ...}}
{:objects {:red_medkit 0.9 :blu_medkit 0.9}}
{:areaA {:location []}}
{:areaB {:location []}}
{:red_medkit {:at {:areaA 0.8 :areaB 0.1 :areaC 0.1}
{:type {:medkit 1.0}}
\end{verbatim}
    \label{fig:surprise_beliefs}
    \caption{Beliefs}
  \end{subfigure}
  \caption{TODO}
  \label{fig:surprise}
\end{figure}


\subsection{Entropy Reduction}
\begin{figure}
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
self:  where is the red_medkit?
cmdrX: i don't know.
self:  cmdrY, where is the red_medkit?
cmdrY: areaB.
self:  okay.
\end{verbatim}
    \label{fig:er_text}
    \caption{Dialog}
  \end{subfigure}
  ~\quad
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
\begin{verbatim}
{{:self {:confidence 0.9}
        {:at ...}}
 {:objects [:red_medkit 
            :blu_medkit]}
 {:red_medkit
   {:at  [:areaA 0.8
          :areaB 0.1
          :areaC 0.1]
    :type :medkit}}
 {:blu_medkit
   {:at  [:areaA 0.8
          :areaB 0.1
          :areaC 0.1]
    :type :medkit}}}
\end{verbatim}
    \label{fig:er_beliefs}
    \caption{Beliefs}
  \end{subfigure}
  \caption{TODO}
  \label{fig:er}
\end{figure}


\subsection{Analysis}
\label{sec:analysis}
% Discuss the advantages and possible disadvantages of your algorithm
% Suggest additional experiments and/or improvements to the solution.

\section{Conclusion}
\label{sec:conclusion}
% Summarize what your solution achieved, and how your algoritm could be
% extended or used in other tasks Briefly discuss directions for future
% work.


Utilizing more types of probabilistic distributions would allow for
more robust inference processes. In this study, only a Bernoulii
distribution and the more general Categorical distribution were
used. This is quite limiting when reasoning about objects in space,
e.g., the location of a given medical kit. A better approach would be
to use continuous distributions, perhaps backed by a particle filter.
Another direction would be to direct dialog to increase common
ground between all agents in the environment. For example, during the
dialog, after the robotic agent first received the location of the
red medical kit from cmdrX, it could have tried to create common
ground with cmdrY, creating an immediate contradiction between the
beliefs of cmdrX and cmdrY. Dialog could then pursue a resolution to
this dialog. A final improvement could be performed during the
grounding of ambiguous referents. In this example, the robot simply
chooses a feature with separate values to attempt to disambiguate the
potential referents. A more natural approach would be to choose the
most salient features of the objects for comparison. This would
require having beliefs about objects and their salient features, but
it is a natural fit for this kind of belief framework.

This paper showed\dots.
\bibliographystyle{apalike} {\small \bibliography{references}}
\end{document}
% LocalWords:  disfluencies disfluency incrementality  cmdrY
% LocalWords:  cmdrX situatedness underspecified
